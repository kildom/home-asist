{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6189bc0e",
   "metadata": {},
   "source": [
    "# Initial experiments with embeddings model\n",
    "\n",
    "This workbook was used for initial setup of the models. Workbook downloads model, converts it, evaluates it, check if it is mattiching current implementation, and finaly saves it as GZipped JSON file that will be pushed to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_expected_venv():\n",
    "    venv_path = Path(\"../.venv\").resolve()\n",
    "    current_prefix = Path(sys.prefix).resolve()\n",
    "    req_file = Path(\"../requirements.txt\").resolve()\n",
    "    if current_prefix == venv_path:\n",
    "        print(\"✅ Running inside ../.venv\")\n",
    "    else:\n",
    "        print(\"❌ Not running inside ../.venv\")\n",
    "        if not venv_path.exists():\n",
    "            print(f\"Creating virtual environment at {venv_path}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"venv\", str(venv_path)])\n",
    "        raise EnvironmentError(\"Please activate the virtual environment located at ../.venv\")\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--dry-run\", \"-r\", str(req_file)], capture_output=True, text=True, check=True)\n",
    "    if re.search(r'Collecting|Using cached|Would install', result.stdout):\n",
    "        print(\"❌ Some required packages are missing. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)])\n",
    "        raise EnvironmentError(\"Modules installed. Re-run the script.\")\n",
    "    else:\n",
    "        print(\"✅ Modules available\")\n",
    "    \n",
    "ensure_expected_venv()\n",
    "\n",
    "sys.path.insert(0, '../train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad256bdf",
   "metadata": {},
   "source": [
    "### Download from hub (or load cached file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from pathlib import Path\n",
    "\n",
    "SAVED_MODEL_DIR = Path('../data/models/saved_speech_embedding_model')\n",
    "\n",
    "if SAVED_MODEL_DIR.exists():\n",
    "    loaded_model = tf.saved_model.load(SAVED_MODEL_DIR)\n",
    "else:\n",
    "    loaded_model = hub.load('https://tfhub.dev/google/speech_embedding/1')\n",
    "    tf.saved_model.save(loaded_model, SAVED_MODEL_DIR, signatures=loaded_model.signatures)\n",
    "\n",
    "loaded_model_default = loaded_model.signatures[\"default\"]\n",
    "\n",
    "print('Signatures:', list(loaded_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddf7cd",
   "metadata": {},
   "source": [
    "### Convert to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84594169",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_MODEL_FILE = Path('../data/models/embedding_model.tflite')\n",
    "\n",
    "if not TFLITE_MODEL_FILE.exists():\n",
    "    converter = tf.lite.TFLiteConverter.from_concrete_functions([loaded_model_default])\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    TFLITE_MODEL_FILE.write_bytes(converter.convert())\n",
    "\n",
    "print(f'\\nYou can inspect {TFLITE_MODEL_FILE.resolve()} with https://netron.app/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937e3d8",
   "metadata": {},
   "source": [
    "### Inspect in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"../logdir\")\n",
    "with writer.as_default():\n",
    "    tf.summary.graph(loaded_model_default.graph)\n",
    "writer.close()\n",
    "print ('RUN IN TERMINAL:\\n', 'tensorboard --logdir=./logdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2727",
   "metadata": {},
   "source": [
    "### Detect windowing and sliding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53906f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(start, end, callback):\n",
    "    low = start\n",
    "    high = end\n",
    "    while low < high:\n",
    "        mid = (low + high) // 2\n",
    "        try:\n",
    "            ok = callback(mid)\n",
    "        except:\n",
    "            ok = False\n",
    "        if ok:\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid + 1\n",
    "    return low if low < end else None\n",
    "\n",
    "min_samples = binary_search(12400, 200000, lambda x: int(tf.size(loaded_model_default(tf.random.uniform(shape=(1, x), dtype=tf.float32))['default'])) > 0)\n",
    "after_step1_samples = binary_search(min_samples, 200000, lambda x: int(tf.size(loaded_model_default(tf.random.uniform(shape=(1, x), dtype=tf.float32))['default'])) > 96)\n",
    "after_step2_samples = binary_search(min_samples, 200000, lambda x: int(tf.size(loaded_model_default(tf.random.uniform(shape=(1, x), dtype=tf.float32))['default'])) > 2 * 96)\n",
    "after_step3_samples = binary_search(min_samples, 200000, lambda x: int(tf.size(loaded_model_default(tf.random.uniform(shape=(1, x), dtype=tf.float32))['default'])) > 3 * 96)\n",
    "step_samples = after_step1_samples - min_samples\n",
    "print(f'Minimum samples: {min_samples} samples = {min_samples / 16} ms')\n",
    "print(f'Sliding step: {step_samples} samples = {step_samples / 16} ms')\n",
    "print(f'Sliding step: {after_step2_samples - after_step1_samples} samples = {(after_step2_samples - after_step1_samples) / 16} ms')\n",
    "print(f'Sliding step: {after_step3_samples - after_step2_samples} samples = {(after_step3_samples - after_step2_samples) / 16} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d771d",
   "metadata": {},
   "source": [
    "### Compare my mel spectrogram with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9628faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mel import calc_mel\n",
    "\n",
    "min_samples = 12400\n",
    "TFLITE_MODEL_FILE = Path('../data/models/embedding_model.tflite')\n",
    "data = np.random.uniform(size=(min_samples), low=-1.0, high=1.0).astype(np.float32)\n",
    "my_mel = calc_mel(data)\n",
    "\n",
    "model_tflite = tf.lite.Interpreter(model_path=str(TFLITE_MODEL_FILE), experimental_preserve_all_tensors=True)\n",
    "model_tflite.resize_tensor_input(\n",
    "    model_tflite.get_input_details()[0]['index'],\n",
    "    [1, min_samples],\n",
    "    strict=True)\n",
    "model_tflite.allocate_tensors()\n",
    "model_tflite_in = model_tflite.get_input_details()[0]['index']\n",
    "model_tflite_out = model_tflite.get_output_details()[0]['index']\n",
    "model_tflite.set_tensor(model_tflite_in, data.reshape(1, -1))\n",
    "model_tflite.invoke()\n",
    "\n",
    "tensor_details = model_tflite.get_tensor_details()\n",
    "\n",
    "for d in tensor_details:\n",
    "    if d['name'] == 'Squeeze':\n",
    "        model_mel = model_tflite.get_tensor(d['index'])\n",
    "        break\n",
    "else:\n",
    "    raise ValueError('\"Squeeze\" tensor not found in TFLite model')\n",
    "\n",
    "print(f'Mel shape (my): {my_mel.shape}')\n",
    "print(f'Mel shape (TFLite): {model_mel.shape}')\n",
    "\n",
    "diff = my_mel - model_mel.reshape(my_mel.shape)\n",
    "print(f'Max diff: {np.max(np.abs(diff))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97076a",
   "metadata": {},
   "source": [
    "### Write JSON model and check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.model_desc as desc\n",
    "import gzip\n",
    "\n",
    "for i in range(10):\n",
    "    desc.self_test()\n",
    "\n",
    "JSON_MODEL_FILE = Path('../models/embedding_model.json.gz')\n",
    "JSON_MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "with gzip.open(JSON_MODEL_FILE, \"wt\", encoding=\"utf-8\", compresslevel=9) as f:\n",
    "    f.write(desc.as_json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
